---
title: "Wine quality predictions with *xgboost* and *plsRglm*"
output: pdf_document
author: Mike Hay
email: mhay@protonmail.com
---

## How to run
This is all done on mixed red and white wines. The pre-pre-processing is done by ```load_wines```, which takes a dataframe of reds and a dataframe of whites, and joins them, indicating the provenance of each row with a ```color``` column, taking character values of either ```red``` or ```white```.

Let ```newdata``` be some combined wine data, like that produced by ```load_wines```. To predict on the pre-trained best model, run the following:
```{r eval=FALSE, include=TRUE}
# Loads the best model, expected to be saved at ./fitted_model.RData
source("./wine_analysis.R")
# Uncomment to load all wines from "./data"
# wines <- load_wines()
# newdata <- split(wines, holdout_prop = 0.2)$testing
predict(model, newdata = newdata)
```
The output should be a numeric vector of predicted wine qualities.

## Data exploration

The data are subjective vinho verde quality judgments (1-10) and physical data for each wine. Most are white vinho verdes, but a minority are reds, which is apparently a thing (most vinho verdes almost look a bit greenish, but the name refers to its age, not its literal color). Below are pair plots of some of the more important columns.

```{r message=FALSE, warning=FALSE, paged.print=FALSE, echo=FALSE, results='hide'}
# preliminaries
library(lemon)
source("./wine_analysis.R")
wines <- load_wines()
tt_split <- split(wines)

wine_pairings(wines)
```
Most variables aren't well-correlated with with ```quality```. The big exception is alcohol. Density is negatively correlated with quality. This is probably because alcohol inversely related to density, everything else being equal. Red and whites are marginally distributed quite differently for the most part. The exception is with alcohol and quality, where they both overlap well - with higher alcohol being higher quality. Red wine, white wine, whatever - people just love alcohol.

There are some outliers in the data. In particular, the ```quality``` ratings are heavily and inconveniently concentrated at 5, 6, and 7, with very few being outside this range. We don't remove the outliers, because they are valid data, and the ensembling should handle them relatively well. Because the untransformed wine data is fed into quantile-based *xgboost*, scaling the predictive features is unncessary. I don't really have any manual feature engineering ideas for these data that would improve on the features derived by *xgboost*. Finally, we leave the dependent variable, ```quality```, alone, because there's no accounting for it.

## Model stacking and ensembling

PLS and other linear methods do poorly on this dataset. To get around this, we need to find a latent feature space where PLS can do a good job. Gradient boosting works well for these kind of problems, so I used an ensemble of *xgboost* models. The job of the PLS is then to find a transformation from the ensemble prediction features to actual ordinal wine quality predictions.

Each gradient booster in the ensemble can be seen as a single transformed feature from the kernel implicitly defined over the distribution of possible boosters trained on the training data - of which our ensemble is a finite-dimensional approximation.

Because they are all chasing the same objective, these models tend to be quite correlated, negating the ensemble's goal of providing diverse predictors. To combat this problem, each *xgboost* model is trained on a "smoothed bootstrap" resample of the original training data. This is accomplished by sampling training data weights from the Dirichlet distribution, parameterized by (per-dimension) concentrations $\alpha$. The Dirichlet distribution has support over n-dimensional simplices where the components of every point sum to unity - or multinomial distributions. These probabilities are rescaled so the average weight is unity.

The reasoning behind Dirichlet reweighting is that regular bootstrapping adds too much stochasticity, and we can't really adjust smoothly. Stochasticity makes each model strictly worse in itself, so we don't want more than we need. Due to the central limit theorem, this isn't much different from adding normal multiplicative noise, but it is still a bit nicer.

After training the *xgboost* models, we are left with a set of transformed features formed by each model's prediction over the training set. The sample variance of these individual predictions varies greatly from point to point in the feature space. This goes against the homoskedasticity assumption of partial least squares. Fortunately, *plsRglm* takes an example weight vector as an option parameter. The model sets this parameter to the inverse variance, or precision, of the sample predictive distribution of each point. This is basically the same thing as weighted least squares. The reweighting also has the effect of reducing the influence of uncertain predictions from our *xgboost* ensemble.

We just use regression in our ensemble, even though ```quality``` is ordinal rather than continuous. However, this is less relevant, because we are not using it to make a final prediction.

The next step is to feed our transformed and reweighted features into *plsRglm*. The big choice here is which *modele* to use. Ordinal regression (```modele="pls-glm-polr"```) makes sense if we want to directly predict valid integer quality values. Arguments could be made for regression models: While all of our observed qualities are integral, one could imagine that they are the product of continuous latent quality judgments in taster's heads, censored to 1-10 values. I used *polr* because it doesn't matter much one way or the other.

The PLS optimization crashes when the data are too co-linear. The Dirichlet noising comes to the rescue here, allowing us to noise each *xgboost* model's objective such that their predictions are sufficiently distinct. This puts a floor on the minimum noise we can add. Another option would be to train the PLS on the the biggest principle components of the ensemble's output. However, this doesn't buy us anything due to the kind of spectra we get: There is always a big first component associated most closely with the training objective. The rest of the eigenvalues decay slowly and linearly, not exponentially. Thus, cutting them off more or less negates the purpose of ensembling, and doesn't help the *plsRglm*'s numerical stability as much as one would think. Kernel PCA might work better, but I haven't tried it.

## Data splitting and cross-validation

Hyperparameter optimization is done by a grid search and 5-fold cross validation over the training data. I found it easier to just write my own cross-validation scheme instead of trying to fit this into caret. The search is performed jointly over both layers. The evaluation metric over each fold is the final mean absolute error (MAE) of the PLS prediction over the out-of-fold segment. In other words, we don't care about the *xgboost* output in itself during cross validation, because we only care about the fidelity of the PLS predictions.

The grid search optimizes the following boosting parameters: ```l1_alpha``` L1 weight regularization,  ```lambda``` L2 weight regularization, ```alpha``` Dirichlet multiplicative noising, and ```colsample_bytree```, the ratio of columns each tree is trained on. The search is also done over ```nt```, the number of retained PLS components.

## Results

These are the best few sets of parameters from the cross-validation, along with their in-fold and out-of-fold (oof) metrics for both the *xgBoost* ensemble and *plsRglm* layers:
```{r echo=FALSE, message=FALSE, warning=FALSE, render=lemon_print}
cv_results <- read.csv("cv_results.csv")
collected <- collect_folds(cv_results) %>% select(-c(n_models, max_depth, colsample_bytree, nrounds, lambda, nt, l1_alpha))
collected[1:3,]

```

It turns out that this is incredibly insensitive to anything other than the Dirichlet $\alpha$ parameter. This is maybe not too much of a surprise: the multiplicative noising is invariant under different models as well as the units of the input data. This is similar to dropout in neural networks - which also works well because it is invariant in internal weights, which have no clear units. The best values seem to be on the order of $1e1$ or $1e0$, with the accuracy seemingly flat around there, with near-zero curvature with respect to all parameters.

Here are the test and training set metrics of the predictions, fitted on the best cross-valided parameters.
```{r echo=FALSE, message=FALSE, warning=FALSE, include=FALSE, render=lemon_print}
best_model <- fit_best(cv_results, data = tt_split$training)
m <- nice_metrics(best_model, tt_split)
```

```{r echo=FALSE, message=FALSE, warning=FALSE, render=lemon_print}
#knittr oddity
m
```
The 0-1 label test accuracy comes in at around 65% to 67%, with the test MAE hovering around 0.36 to 0.38. There is a bigger difference between the performance on testing and training data than I usually like to see. This is probably nearly as good as could reasonably be done on this dataset, given its limited size and subjective nature.

As a final thing, PLS tries to find a lower-dimensional embedding explaining as much of the dependent variable variance as possible. The following plot is the projection of the testing data, through the stack, onto the first two dimensions of *plsRglm*'s transformation. The points are colored by ```quality```. Misclassified ones are given by circles.

```{r message=FALSE, warning=FALSE, paged.print=TRUE, echo = FALSE}
transformed <- transform(best_model, tt_split$testing)[,1:2]
quality = tt_split$testing$quality
is_correct <- predict(best_model, tt_split$testing) == quality
plt_df <- tibble(first = transformed[,1],
                 second = transformed[,2],
                 quality = factor(quality),
                 correct = is_correct)
ggplot(plt_df) +
  geom_point(size = 1.5,
             aes(x = first, y = second, color=quality,  shape=correct)) +
  scale_color_brewer(palette="Set1")
```